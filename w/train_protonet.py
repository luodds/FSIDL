# train_protonet.py

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

def preprocess_data(file_path):
    """
    åŠ è½½ã€æ¸…æ´—å¹¶é¢„å¤„ç†ç½‘ç»œæµé‡æ•°æ®é›†ã€‚
    (æœ€ç»ˆç‰ˆæœ¬ - åŒ…å«äº†æ‚¨æŒ‡å®šçš„ã€éå¸¸è¯¦å°½çš„åˆ—ç§»é™¤åˆ—è¡¨)
    """
    print("--- æ­¥éª¤ 1: å¼€å§‹æ•°æ®é¢„å¤„ç† ---")
    
    try:
        df = pd.read_csv(file_path, low_memory=False)
    except Exception as e:
        print(f"âŒ åŠ è½½æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return None, None, None

    print(f"åŸå§‹æ•°æ®é›†ç»´åº¦: {df.shape}")

    target_column = 'Attack Type'
    if target_column not in df.columns:
        print(f"âŒ é”™è¯¯: ç›®æ ‡åˆ— '{target_column}' ä¸åœ¨CSVæ–‡ä»¶ä¸­ã€‚")
        return None, None, None

    # --- ç¼ºå¤±å€¼å¤„ç† ---
    df.replace([np.inf, -np.inf], np.nan, inplace=True)
    df.dropna(subset=[target_column], inplace=True)
    
    if df.shape[0] == 0:
        print(f"âŒ é”™è¯¯: æ•°æ®é›†å˜ä¸ºç©ºã€‚")
        return None, None, None
    print(f"åˆ é™¤æ ‡ç­¾ç¼ºå¤±å€¼åç»´åº¦: {df.shape}")
    
    # ç¼–ç ç›®æ ‡æ ‡ç­¾
    label_encoder = LabelEncoder()
    y = label_encoder.fit_transform(df[target_column])
    print(f"æ ‡ç­¾ '{target_column}' å·²è¢«ç¼–ç ä¸º {len(np.unique(y))} ä¸ªç±»åˆ«ã€‚")
    print("ç±»åˆ«æ˜ å°„:", {i: c for i, c in enumerate(label_encoder.classes_)})
    
    # --- [æ ¸å¿ƒä¿®æ”¹] æ™ºèƒ½ç‰¹å¾å·¥ç¨‹ä¸é€‰æ‹© ---
    
    # ==================================================================
    # |              >>>>>> ä»¥ä¸‹æ˜¯æ‚¨æŒ‡å®šçš„ä»£ç å— <<<<<<                  |
    # ==================================================================

    # --- 1. å®šä¹‰è¦ç§»é™¤çš„åˆ— ---
    columns_to_drop = [
        # æ ‡è¯†ç¬¦å’Œé«˜åŸºæ•°ç‰¹å¾
        'SrcId', 'SrcAddr', 'DstAddr', 'SrcMac', 'DstMac', 'SrcOui', 'DstOui',
        'sIpId', 'dIpId', 
        
        # ä¹‹å‰åˆ†æå‡ºçš„æ‰€æœ‰å¼ºç›¸å…³ç‰¹å¾ (æ ¹æ®æ‚¨çš„è¦æ±‚æ·»åŠ )
        'IdleTime', 'AckDat', 'DstTCPBase', 'SrcTCPBase', 
        'TcpRtt', 'dHops', 'sHops', 'dTtl', 'Seq', 'Rank', 'Offset', 
        
        
        # å†—ä½™æˆ–ç›´æ¥ç›¸å…³çš„ç‰¹å¾
        'RunTime', 'Label', 'Attack Tool', 'attack_cat',
        
        # åŸå§‹æ—¶é—´æˆ³
        'StartTime', 'LastTime'
    ]
    
    # è¿‡æ»¤æ‰æ•°æ®é›†ä¸­ä¸å­˜åœ¨çš„åˆ—åï¼Œé¿å…å‡ºé”™
    existing_cols_to_drop = [col for col in columns_to_drop if col in df.columns]
    
    # åŒæ ·è¦ç¡®ä¿ç›®æ ‡åˆ—ä¹Ÿè¢«æ’é™¤åœ¨ç‰¹å¾ä¹‹å¤–
    if target_column in existing_cols_to_drop:
        existing_cols_to_drop.remove(target_column)
    if target_column in df.columns:
         df.drop(columns=[target_column], inplace=True)

    print(f"\nè®¡åˆ’ç§»é™¤ {len(existing_cols_to_drop)} ä¸ªç‰¹å¾...")
    df.drop(columns=existing_cols_to_drop, inplace=True, errors='ignore')
    print("ç‰¹å¾ç§»é™¤å®Œæˆã€‚")
    
    # ==================================================================
    # |              >>>>>> ä»£ç å—é›†æˆç»“æŸ <<<<<<                       |
    # ==================================================================

    # 2. å°†æ—¶é—´æˆ³ç­‰ object ç±»å‹çš„åˆ—å¼ºåˆ¶è½¬æ¢ä¸ºæ•°å€¼ï¼Œæ— æ³•è½¬æ¢çš„å˜ä¸º NaN
    for col in df.select_dtypes(include=['object']).columns:
        df[col] = pd.to_numeric(df[col], errors='coerce')

    # 3. ç°åœ¨æ‰€æœ‰åˆ—éƒ½åº”è¯¥æ˜¯æ•°å€¼ç±»å‹äº†ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨0æ¥å¡«å……æ‰€æœ‰å‰©ä½™çš„NaN
    df.fillna(0, inplace=True)
    
    print(f"æ‰€æœ‰ç‰¹å¾å·²è½¬æ¢ä¸ºæ•°å€¼ç±»å‹ã€‚")

    # 4. ç°åœ¨æ‰€æœ‰åˆ—éƒ½æ˜¯æ•°å€¼å‹ï¼Œå¯ä»¥ç›´æ¥è¿›è¡Œå½’ä¸€åŒ–
    scaler = StandardScaler()
    X = scaler.fit_transform(df)

    # ç¡®ä¿æœ€ç»ˆæ•°ç»„ç±»å‹æ­£ç¡®
    X = X.astype(np.float32)
    
    print(f"\né¢„å¤„ç†å®Œæˆã€‚æœ€ç»ˆç‰¹å¾çŸ©é˜µç»´åº¦: {X.shape}")
    print("----------------------------------\n")
    return X, y, label_encoder



# train_protonet.py (æ¥ä¸Šæ–‡)

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Sampler, Dataset

class PrototypicalNetMLP(nn.Module):
    """
    ç”¨äºè¡¨æ ¼æ•°æ®çš„ MLP åµŒå…¥ç½‘ç»œã€‚ (åŠ æ·±ç‰ˆ)
    """
    def __init__(self, input_dim, embedding_dim=64):
        super(PrototypicalNetMLP, self).__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, 128),
            nn.ReLU(),
            nn.BatchNorm1d(128),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.BatchNorm1d(128),
            nn.Linear(128, 128), # æ–°å¢çš„éšè—å±‚
            nn.ReLU(),
            nn.BatchNorm1d(128),
            nn.Linear(128, embedding_dim)
        )
    
    def forward(self, x):
        return self.net(x)

class TrafficDataset(Dataset):
    """ä¸€ä¸ªç®€å•çš„PyTorchæ•°æ®é›†åŒ…è£…å™¨"""
    def __init__(self, data, labels):
        self.data = torch.FloatTensor(data)
        self.labels = torch.LongTensor(labels)

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        return self.data[idx], self.labels[idx]

class EpisodicBatchSampler(Sampler):
    """
    Episodic Batch Sampler.
    ä¸ºæ¯ä¸ª episode (batch) äº§ç”Ÿ N-way K-shot Q-query çš„æ ·æœ¬ç´¢å¼•ã€‚
    """
    def __init__(self, labels, n_episodes, n_way, n_samples):
        super().__init__(labels)
        self.labels = labels
        self.n_episodes = n_episodes
        self.n_way = n_way
        self.n_samples = n_samples # n_support + n_query

        # æŒ‰ç±»åˆ«å¯¹æ ·æœ¬ç´¢å¼•è¿›è¡Œåˆ†ç»„
        self.class_indices = {c: np.where(self.labels == c)[0] for c in np.unique(self.labels)}

    def __len__(self):
        return self.n_episodes

    def __iter__(self):
        for _ in range(self.n_episodes):
            episode_indices = []
            # éšæœºé€‰æ‹© N ä¸ªç±»åˆ«
            available_classes = [c for c, indices in self.class_indices.items() if len(indices) >= self.n_samples]
            if len(available_classes) < self.n_way:
                continue # å¦‚æœæ²¡æœ‰è¶³å¤Ÿçš„ç±»æ¥æ„å»ºepisodeï¼Œåˆ™è·³è¿‡
            
            selected_classes = np.random.choice(available_classes, self.n_way, replace=False)
            
            for c in selected_classes:
                # ä»æ¯ä¸ªé€‰å®šç±»åˆ«ä¸­éšæœºé€‰æ‹© n_samples ä¸ªæ ·æœ¬
                class_idx = np.random.choice(self.class_indices[c], self.n_samples, replace=False)
                episode_indices.extend(class_idx)
            
            yield torch.LongTensor(episode_indices)


# train_protonet.py (æ¥ä¸Šæ–‡)

from torch.optim import Adam
from tqdm import tqdm

def prototypical_loss(embeddings, n_support, n_way, n_query):
    """è®¡ç®— Prototypical Loss å’Œ Accuracy"""
    
    # 1. åˆ†ç¦» support å’Œ query æ ·æœ¬çš„åµŒå…¥
    embedding_dim = embeddings.size(-1)
    support_embeddings = embeddings[:n_way * n_support].view(n_way, n_support, embedding_dim)
    query_embeddings = embeddings[n_way * n_support:]

    # 2. è®¡ç®—æ¯ä¸ªç±»çš„åŸå‹
    prototypes = support_embeddings.mean(dim=1)

    # 3. è®¡ç®— query æ ·æœ¬åˆ°æ¯ä¸ªåŸå‹çš„è·ç¦» (å¹³æ–¹æ¬§æ°è·ç¦»)
    # (n_query * n_way, embedding_dim) -> (n_query * n_way, 1, embedding_dim)
    # (n_way, embedding_dim) -> (1, n_way, embedding_dim)
    # å¹¿æ’­åç›¸å‡
    distances = (query_embeddings.unsqueeze(1) - prototypes.unsqueeze(0)).pow(2).sum(dim=2)

    # 4. è®¡ç®—æŸå¤±
    # å°†è·ç¦»è½¬æ¢ä¸ºè´Ÿå¯¹æ•°æ¦‚ç‡
    log_p_y = F.log_softmax(-distances, dim=1)
    
    # ç”Ÿæˆ query set çš„çœŸå®æ ‡ç­¾
    query_labels = torch.arange(n_way).repeat_interleave(n_query)
    
    # ä½¿ç”¨è´Ÿå¯¹æ•°ä¼¼ç„¶æŸå¤± (NLL Loss)
    loss = F.nll_loss(log_p_y, query_labels)

    # 5. è®¡ç®—å‡†ç¡®ç‡
    y_hat = log_p_y.argmax(dim=1)
    acc = (y_hat == query_labels).float().mean()
    
    return loss, acc


from torch.optim import Adam
from tqdm import tqdm
# [æ–°å¢] å¯¼å…¥å¯è§†åŒ–åº“
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE

def main():
    # ... (mainå‡½æ•°å‰é¢çš„æ‰€æœ‰ä»£ç ï¼ŒåŒ…æ‹¬è®­ç»ƒå’Œè¯„ä¼°ï¼Œéƒ½ä¿æŒä¸å˜) ...
    # --- è¶…å‚æ•°é…ç½® ---
    N_WAY = 5
    K_SHOT = 5
    N_QUERY = 10
    N_TRAIN_EPISODES = 5000 # ä½¿ç”¨å¢åŠ åçš„è®­ç»ƒé‡
    N_TEST_EPISODES = 200
    EMBEDDING_DIM = 64
    LEARNING_RATE = 0.001

    # --- 1. æ•°æ®å‡†å¤‡ ---
    file_path = 'w/merged_all_attacks.csv'
    X, y, label_encoder = preprocess_data(file_path)

    if X is None:
        return 

    num_classes = len(np.unique(y))
    if num_classes < N_WAY:
        print(f"âš ï¸ æ•°æ®é›†ç±»åˆ«æ•° ({num_classes}) å°‘äº N_WAY ({N_WAY})ã€‚")
        N_WAY = num_classes
        print(f"   å·²è‡ªåŠ¨å°† N_WAY è°ƒæ•´ä¸º: {N_WAY}")

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    train_dataset = TrafficDataset(X_train, y_train)
    test_dataset = TrafficDataset(X_test, y_test)

    # --- 2. æ¨¡å‹å’Œä¼˜åŒ–å™¨ ---
    print("\n--- æ­¥éª¤ 2: åˆå§‹åŒ–æ¨¡å‹å’Œä¼˜åŒ–å™¨ ---")
    input_dim = X_train.shape[1]
    model = PrototypicalNetMLP(input_dim, embedding_dim=EMBEDDING_DIM)
    optimizer = Adam(model.parameters(), lr=LEARNING_RATE)
    print(model)
    print("----------------------------------\n")

    # --- 3. è®­ç»ƒ ---
    print("\n--- æ­¥éª¤ 3: å¼€å§‹è®­ç»ƒ ---")
    # ... (è®­ç»ƒå¾ªç¯ä»£ç ä¸å˜) ...
    train_sampler = EpisodicBatchSampler(y_train, N_TRAIN_EPISODES, N_WAY, K_SHOT + N_QUERY)
    train_losses = []
    train_accuracies = []
    model.train()
    for episode_indices in tqdm(train_sampler, desc="Training Episodes"):
        optimizer.zero_grad()
        data, _ = train_dataset[episode_indices]
        embeddings = model(data)
        loss, acc = prototypical_loss(embeddings, n_support=K_SHOT, n_way=N_WAY, n_query=N_QUERY)
        loss.backward()
        optimizer.step()
        train_losses.append(loss.item())
        train_accuracies.append(acc.item())
    print("è®­ç»ƒå®Œæˆã€‚")
    print("----------------------------------\n")
    
    # --- 4. è¯„ä¼° ---
    print("\n--- æ­¥éª¤ 4: å¼€å§‹è¯„ä¼° ---")
    test_sampler = EpisodicBatchSampler(y_test, N_TEST_EPISODES, N_WAY, K_SHOT + N_QUERY)
    
    model.eval()
    
    # [æ–°å¢] åˆ›å»ºä¸€ä¸ªå­—å…¸æ¥å­˜å‚¨æ¯ä¸ªepisodeçš„é¢„æµ‹å’ŒçœŸå®æ ‡ç­¾
    # è¿™æ ·å¯ä»¥å¤„ç†æ¯ä¸ªepisodeç±»åˆ«éƒ½ä¸åŒçš„æƒ…å†µ
    episode_results = {'preds': [], 'labels': [], 'class_map': []}
    
    # è·å–æ‰€æœ‰å¯èƒ½çš„ç±»åˆ«æ ‡ç­¾çš„æ•°å€¼ç¼–ç 
    all_possible_labels = np.arange(len(label_encoder.classes_))
    
    total_acc = 0.0

    with torch.no_grad():
        for episode_indices in tqdm(test_sampler, desc="Testing Episodes"):
            data, true_episode_labels_flat = test_dataset[episode_indices]
            
            # ç¡®å®šå½“å‰episodeä¸­ç‹¬ç‰¹çš„ç±»åˆ«åŠå…¶åœ¨åŸå§‹ç¼–ç ä¸­çš„å€¼
            # ä¾‹å¦‚: [Benign, SYNFlood, UDPScan] -> [0, 3, 7]
            true_episode_classes = np.unique(true_episode_labels_flat.numpy())
            
            embeddings = model(data)
            
            embedding_dim = embeddings.size(-1)
            support_embeddings = embeddings[:N_WAY * K_SHOT].view(N_WAY, K_SHOT, embedding_dim)
            query_embeddings = embeddings[N_WAY * K_SHOT:]
            prototypes = support_embeddings.mean(dim=1)
            distances = (query_embeddings.unsqueeze(1) - prototypes.unsqueeze(0)).pow(2).sum(dim=2)
            
            # é¢„æµ‹çš„æ ‡ç­¾æ˜¯ç›¸å¯¹äºå½“å‰ episode çš„ç´¢å¼• (0, 1, 2, 3, 4)
            predictions_in_episode_idx = (-distances).argmax(dim=1)
            
            # å°† episode å†…çš„ç´¢å¼•æ˜ å°„å›åŸå§‹çš„ç±»åˆ«ç¼–ç 
            # ä¾‹å¦‚ï¼Œå¦‚æœ episode å†…é¢„æµ‹ä¸º 1ï¼Œä¸”å½“å‰ episode çš„ç±»åˆ«æ˜¯ [0, 3, 7]ï¼Œé‚£ä¹ˆé¢„æµ‹ 1 å¯¹åº”åŸå§‹ç±»åˆ« 3
            predicted_original_labels = torch.tensor([true_episode_classes[i] for i in predictions_in_episode_idx])
            
            # è·å– query set çš„çœŸå®æ ‡ç­¾ (åŸå§‹ç¼–ç )
            true_query_original_labels = true_episode_labels_flat[N_WAY * K_SHOT:]

            episode_results['preds'].extend(predicted_original_labels.numpy())
            episode_results['labels'].extend(true_query_original_labels.numpy())

            # è®¡ç®—å‡†ç¡®ç‡
            acc = (predicted_original_labels == true_query_original_labels).float().mean()
            total_acc += acc.item()
            
    avg_acc = total_acc / N_TEST_EPISODES
    print(f"\nè¯„ä¼°ç»“æœ: å¹³å‡å‡†ç¡®ç‡: {avg_acc * 100:.2f}%")
    
    # [æ ¸å¿ƒä¿®æ”¹] åœ¨è®¡ç®—æ··æ·†çŸ©é˜µæ—¶ï¼Œæä¾›å®Œæ•´çš„ç±»åˆ«åˆ—è¡¨
    cm = confusion_matrix(
        episode_results['labels'], 
        episode_results['preds'], 
        labels=all_possible_labels  # <-- å…³é”®ä¿®æ”¹ï¼
    )
    
    disp = ConfusionMatrixDisplay(
        confusion_matrix=cm, 
        display_labels=label_encoder.classes_  # <-- è¿™é‡Œçš„æ ‡ç­¾æ•°é‡ç°åœ¨å’Œ cm çš„ç»´åº¦åŒ¹é…äº†
    )
    
    fig, ax = plt.subplots(figsize=(10, 10))
    disp.plot(ax=ax, xticks_rotation='vertical', cmap='Blues')
    plt.title("Confusion Matrix")
    plt.tight_layout()
    plt.savefig("confusion_matrix.png")
    print("ğŸ”¢ æ··æ·†çŸ©é˜µå·²ä¿å­˜ä¸º 'confusion_matrix.png'")
    plt.show()

    # --- 5. å¯è§†åŒ– ---
    print("\n--- æ­¥éª¤ 5: å¼€å§‹å¯è§†åŒ– ---")
    # ... (è®­ç»ƒæ›²çº¿ç»˜åˆ¶éƒ¨åˆ†ä»£ç ä¸å˜) ...
    plt.figure(figsize=(12, 5))
    plt.subplot(1, 2, 1)
    plt.plot(train_losses)
    plt.title("Training Loss per Episode")
    plt.xlabel("Episode")
    plt.ylabel("Loss")
    plt.subplot(1, 2, 2)
    plt.plot(train_accuracies)
    plt.title("Training Accuracy per Episode")
    plt.xlabel("Episode")
    plt.ylabel("Accuracy")
    plt.tight_layout()
    plt.savefig("training_curves.png")
    print("ğŸ“ˆ è®­ç»ƒæ›²çº¿å·²ä¿å­˜ä¸º 'training_curves.png'")
    plt.show()

    print("ğŸ¨ æ­£åœ¨ç”Ÿæˆ t-SNE åµŒå…¥ç©ºé—´å¯è§†åŒ–...")
    n_samples_for_tsne = 1000 
    test_subset_indices = np.random.choice(len(test_dataset), n_samples_for_tsne, replace=False)
    test_subset_data, test_subset_labels = test_dataset[test_subset_indices]

    with torch.no_grad():
        test_embeddings = model(test_subset_data).numpy()
    
    tsne = TSNE(n_components=2, random_state=42, perplexity=30, max_iter=1000)
    tsne_results = tsne.fit_transform(test_embeddings)
    
    plt.figure(figsize=(10, 8))
    scatter = plt.scatter(tsne_results[:, 0], tsne_results[:, 1], c=test_subset_labels, cmap='viridis', alpha=0.7)
    
    # [æ ¸å¿ƒä¿®æ”¹] å°† NumPy æ•°ç»„è½¬æ¢ä¸º Python åˆ—è¡¨
    legend_labels = label_encoder.inverse_transform(np.unique(test_subset_labels))
    plt.legend(handles=scatter.legend_elements()[0], labels=legend_labels.tolist(), title="Classes")
    
    plt.title("t-SNE Visualization of Test Set Embeddings")
    plt.xlabel("t-SNE Dimension 1")
    plt.ylabel("t-SNE Dimension 2")
    plt.savefig("embedding_space_tsne.png")
    print("ğŸ–¼ï¸ t-SNE å¯è§†åŒ–å›¾å·²ä¿å­˜ä¸º 'embedding_space_tsne.png'")
    plt.show()

if __name__ == '__main__':
    main()